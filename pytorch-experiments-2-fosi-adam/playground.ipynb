{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "view-in-github"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/PavlosPo/nlp-optimizers/blob/pavlos-playground/pytorch-experiments-1-fosi-adam/playground.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JVmTDAZPH2-f"
      },
      "source": [
        "## FOSI Classes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pD-T81dbHzYD",
        "outputId": "f92a1b0f-2b0b-4983-ae7c-cbee3b1ff7ff"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Archive:  fosi.zip\n",
            " extracting: ./fosi/__init__.py      \n",
            "   creating: ./fosi/jax_optim/\n",
            " extracting: ./fosi/jax_optim/__init__.py  \n",
            " extracting: ./fosi/jax_optim/extreme_spectrum_estimation.py  \n",
            " extracting: ./fosi/jax_optim/fosi_optimizer.py  \n",
            " extracting: ./fosi/jax_optim/lanczos_algorithm.py  \n",
            " extracting: ./fosi/jax_optim/lanczos_algorithm_sanity.py  \n",
            "   creating: ./fosi/torch_optim/\n",
            " extracting: ./fosi/torch_optim/__init__.py  \n",
            " extracting: ./fosi/torch_optim/extreme_spectrum_estimation.py  \n",
            " extracting: ./fosi/torch_optim/fosi_optimizer.py  \n",
            " extracting: ./fosi/torch_optim/lanczos_algorithm.py  \n",
            " extracting: ./fosi/torch_optim/lanczos_algorithm_sanity.py  \n",
            " extracting: ./fosi/version.py       \n"
          ]
        }
      ],
      "source": [
        "# ! rm -rf ./fosi/\n",
        "# !mkdir ./fosi/\n",
        "# !unzip fosi.zip -d ./fosi/"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZYKw68tXH0Ue"
      },
      "source": [
        "## Working Example"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZtEnvL7sHwCC",
        "outputId": "27e6e68c-4c93-49e5-e04c-676253a35000"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: torchopt in /usr/local/lib/python3.10/dist-packages (0.7.3)\n",
            "Requirement already satisfied: torch>=1.13 in /usr/local/lib/python3.10/dist-packages (from torchopt) (2.2.1+cu121)\n",
            "Requirement already satisfied: optree>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from torchopt) (0.10.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from torchopt) (1.25.2)\n",
            "Requirement already satisfied: graphviz in /usr/local/lib/python3.10/dist-packages (from torchopt) (0.20.2)\n",
            "Requirement already satisfied: typing-extensions>=4.0.0 in /usr/local/lib/python3.10/dist-packages (from torchopt) (4.10.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch>=1.13->torchopt) (3.13.1)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=1.13->torchopt) (1.12)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.13->torchopt) (3.2.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.13->torchopt) (3.1.3)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch>=1.13->torchopt) (2023.6.0)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch>=1.13->torchopt) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch>=1.13->torchopt) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch>=1.13->torchopt) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==8.9.2.26 in /usr/local/lib/python3.10/dist-packages (from torch>=1.13->torchopt) (8.9.2.26)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in /usr/local/lib/python3.10/dist-packages (from torch>=1.13->torchopt) (12.1.3.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in /usr/local/lib/python3.10/dist-packages (from torch>=1.13->torchopt) (11.0.2.54)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.2.106 in /usr/local/lib/python3.10/dist-packages (from torch>=1.13->torchopt) (10.3.2.106)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in /usr/local/lib/python3.10/dist-packages (from torch>=1.13->torchopt) (11.4.5.107)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in /usr/local/lib/python3.10/dist-packages (from torch>=1.13->torchopt) (12.1.0.106)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.19.3 in /usr/local/lib/python3.10/dist-packages (from torch>=1.13->torchopt) (2.19.3)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch>=1.13->torchopt) (12.1.105)\n",
            "Requirement already satisfied: triton==2.2.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.13->torchopt) (2.2.0)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12 in /usr/local/lib/python3.10/dist-packages (from nvidia-cusolver-cu12==11.4.5.107->torch>=1.13->torchopt) (12.4.99)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.13->torchopt) (2.1.5)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=1.13->torchopt) (1.3.0)\n"
          ]
        }
      ],
      "source": [
        "# !pip install torchopt\n",
        "# !pip install datasets\n",
        "# !pip install evaluate"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "VVIyQm1XFvhQ"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torchvision.transforms as transforms\n",
        "from torch.utils.data.dataloader import default_collate\n",
        "from transformers import AutoModelForSequenceClassification, AutoTokenizer, DataCollatorWithPadding, Trainer, TrainingArguments\n",
        "from torch.optim import Adam\n",
        "import torch.nn as nn\n",
        "import torchopt\n",
        "import functorch\n",
        "import evaluate\n",
        "import torch.nn.functional as F\n",
        "\n",
        "from datasets import load_dataset\n",
        "from fosi import fosi_adam_torch\n",
        "\n",
        "# # Set device\n",
        "# device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
        "\n",
        "# # Load pre-trained DistilBERT model and tokenizer\n",
        "# base_model = AutoModelForSequenceClassification.from_pretrained('bert-base-uncased', num_labels=2)\n",
        "# base_model.to(device)\n",
        "# tokenizer = AutoTokenizer.from_pretrained('bert-base-uncased')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "vB2RtawpGA8R"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "/home/pavpoulos/miniconda3/envs/pavlosEnv/lib/python3.12/site-packages/torch/cuda/__init__.py:141: UserWarning: CUDA initialization: The NVIDIA driver on your system is too old (found version 11040). Please update your GPU driver by downloading and installing a new version from the URL: http://www.nvidia.com/Download/index.aspx Alternatively, go to: https://pytorch.org to install a PyTorch version that has been compiled with your version of the CUDA driver. (Triggered internally at ../c10/cuda/CUDAFunctions.cpp:108.)\n",
            "  return torch._C._cuda_getDeviceCount() > 0\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "BertClassifier(\n",
              "  (bert): BertForSequenceClassification(\n",
              "    (bert): BertModel(\n",
              "      (embeddings): BertEmbeddings(\n",
              "        (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
              "        (position_embeddings): Embedding(512, 768)\n",
              "        (token_type_embeddings): Embedding(2, 768)\n",
              "        (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "        (dropout): Dropout(p=0.1, inplace=False)\n",
              "      )\n",
              "      (encoder): BertEncoder(\n",
              "        (layer): ModuleList(\n",
              "          (0-11): 12 x BertLayer(\n",
              "            (attention): BertAttention(\n",
              "              (self): BertSelfAttention(\n",
              "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "                (dropout): Dropout(p=0.1, inplace=False)\n",
              "              )\n",
              "              (output): BertSelfOutput(\n",
              "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "                (dropout): Dropout(p=0.1, inplace=False)\n",
              "              )\n",
              "            )\n",
              "            (intermediate): BertIntermediate(\n",
              "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "              (intermediate_act_fn): GELUActivation()\n",
              "            )\n",
              "            (output): BertOutput(\n",
              "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "        )\n",
              "      )\n",
              "      (pooler): BertPooler(\n",
              "        (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "        (activation): Tanh()\n",
              "      )\n",
              "    )\n",
              "    (dropout): Dropout(p=0.1, inplace=False)\n",
              "    (classifier): Linear(in_features=768, out_features=1, bias=True)\n",
              "  )\n",
              "  (sigmoid): Sigmoid()\n",
              ")"
            ]
          },
          "execution_count": 4,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from transformers import AutoModelForSequenceClassification, AutoTokenizer, DataCollatorWithPadding\n",
        "\n",
        "# Load pre-trained BERT model and tokenizer\n",
        "bert_model_name = \"bert-base-uncased\"\n",
        "num_classes = 1\n",
        "bert_model = AutoModelForSequenceClassification.from_pretrained(bert_model_name, num_labels=num_classes)\n",
        "tokenizer = AutoTokenizer.from_pretrained(bert_model_name)\n",
        "\n",
        "# # Freeze all parameters of the BERT model except for the last layer (classifier)\n",
        "# for name, param in bert_model.named_parameters():\n",
        "#     if 'classifier' not in name:  # Exclude parameters of the classifier layer\n",
        "#         param.requires_grad = False\n",
        "\n",
        "# Define a dense layer on top of the BERT model for classification\n",
        "class BertClassifier(nn.Module):\n",
        "    def __init__(self, bert_model, num_classes):\n",
        "        super(BertClassifier, self).__init__()\n",
        "        self.bert = bert_model\n",
        "        self.sigmoid = nn.Sigmoid()\n",
        "\n",
        "    def forward(self, input_ids, attention_mask):\n",
        "        outputs = self.bert(input_ids=input_ids, attention_mask=attention_mask).logits\n",
        "        probability = self.sigmoid(outputs)\n",
        "        return probability\n",
        "\n",
        "# Instantiate the classifier\n",
        "classifier = BertClassifier(bert_model, num_classes)\n",
        "\n",
        "# Optionally, move the model to a GPU device if available\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "classifier.to(device)\n",
        "\n",
        "# Now you can train the entire model (BERT + dense layer) on your task-specific data\n",
        "# Make sure to prepare your data (input_ids, attention_mask, labels) using the BERT tokenizer\n",
        "# and use an appropriate loss function and optimizer for your task\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "NEhHBnRiFvhR"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Map: 100%|██████████| 67349/67349 [00:06<00:00, 9627.95 examples/s] \n"
          ]
        }
      ],
      "source": [
        "\n",
        "# Define a function to preprocess the dataset\n",
        "def prepare_dataset(example):\n",
        "    return tokenizer(example['sentence'], add_special_tokens=True, truncation=True, padding=True, return_tensors='pt')\n",
        "\n",
        "data_collator = DataCollatorWithPadding(tokenizer=tokenizer, padding=True)\n",
        "\n",
        "dataset = load_dataset('glue', 'sst2').map(prepare_dataset, batched=True)\n",
        "metric = evaluate.load(\"glue\", \"sst2\")\n",
        "\n",
        "# Split dataset into train and test sets, we use the train category because the test one has labels -1 only.\n",
        "train_dataset = dataset['train'].select(range(0,500)).remove_columns(['sentence', 'idx']).rename_column('label', 'labels')\n",
        "test_dataset = dataset['train'].select(range(500, 1000)).remove_columns(['sentence', 'idx']).rename_column('label', 'labels')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "7zU_KRNimwR1"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "67349"
            ]
          },
          "execution_count": 7,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "len(dataset['train']['label'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "KaC8HrwiFvhS"
      },
      "outputs": [],
      "source": [
        "# Define data loaders\n",
        "batch_size = 128\n",
        "trainloader = torch.utils.data.DataLoader(train_dataset, batch_size=batch_size, shuffle=True, collate_fn=data_collator)\n",
        "testloader = torch.utils.data.DataLoader(test_dataset, batch_size=batch_size, shuffle=True, collate_fn=data_collator)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "ikDlQ1UDJYGT"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor([[  101,  2054,  2017,  ...,     0,     0,     0],\n",
            "        [  101,  1045,  2031,  ...,     0,     0,     0],\n",
            "        [  101,  1037,  4438,  ...,     0,     0,     0],\n",
            "        ...,\n",
            "        [  101,  2659,  2006,  ...,     0,     0,     0],\n",
            "        [  101,  8991,  4818,  ...,     0,     0,     0],\n",
            "        [  101, 14153, 12369,  ...,     0,     0,     0]])\n"
          ]
        }
      ],
      "source": [
        "for i in (testloader):\n",
        "  print(i['input_ids'])\n",
        "  break"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "TO8H7XfbFvhT"
      },
      "outputs": [],
      "source": [
        "# With buffers\n",
        "\n",
        "def loss_fn(functional_model, params, buffers, input_ids, attention_mask, labels):\n",
        "    preds = functional_model(params, buffers=buffers, input_ids=input_ids, attention_mask=attention_mask)\n",
        "    loss = nn.functional.binary_cross_entropy(preds.squeeze().to(torch.float32), labels.squeeze().to(torch.float32))\n",
        "    return loss\n",
        "\n",
        "# def softmax_output_fn(params, buffers, input_ids, attention_mask, labels):\n",
        "#     logits = model(params, buffers=buffers, input_ids=input_ids, attention_mask=attention_mask, labels=labels).logits\n",
        "#     logits_tensor = torch.tensor(logits)\n",
        "#     softmax_output = F.softmax(logits_tensor, dim=1)\n",
        "#     return softmax_output"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sRrhbO-bIJDN"
      },
      "outputs": [],
      "source": [
        "# Without buffers\n",
        "\n",
        "# def loss_fn(params, input_ids, attention_mask, labels):\n",
        "#     loss = model(params, input_ids=input_ids, attention_mask=attention_mask, labels=labels).loss\n",
        "#     return loss\n",
        "\n",
        "# def softmax_output_fn(params, input_ids, attention_mask, labels):\n",
        "#     logits = model(params, input_ids=input_ids, attention_mask=attention_mask, labels=labels).logits\n",
        "#     logits_tensor = torch.tensor(logits)\n",
        "#     softmax_output = F.softmax(logits_tensor, dim=1)\n",
        "#     return softmax_output"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "QU6tGV1toSnb"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "****************************************************************************************************\n",
            "\n",
            "\n",
            "input_ids: tensor([[  101,  2892, 10689,  ...,     0,     0,     0],\n",
            "        [  101, 26202,  2015,  ...,     0,     0,     0],\n",
            "        [  101,  2804,  2000,  ...,     0,     0,     0],\n",
            "        ...,\n",
            "        [  101,  1997,  1996,  ...,     0,     0,     0],\n",
            "        [  101,  3084,  2026,  ...,     0,     0,     0],\n",
            "        [  101,  3073,  2049,  ...,     0,     0,     0]])\n",
            "attention_mask: tensor([[1, 1, 1,  ..., 0, 0, 0],\n",
            "        [1, 1, 1,  ..., 0, 0, 0],\n",
            "        [1, 1, 1,  ..., 0, 0, 0],\n",
            "        ...,\n",
            "        [1, 1, 1,  ..., 0, 0, 0],\n",
            "        [1, 1, 1,  ..., 0, 0, 0],\n",
            "        [1, 1, 1,  ..., 0, 0, 0]])\n",
            "labels: tensor([1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0,\n",
            "        0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1,\n",
            "        0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1,\n",
            "        1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0,\n",
            "        1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1,\n",
            "        0, 0, 0, 1, 1, 1, 0, 1])\n",
            "Returned ESE function. Lanczos order (m) is 20 .\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/pavpoulos/miniconda3/envs/pavlosEnv/lib/python3.12/site-packages/torch/_functorch/deprecated.py:104: UserWarning: We've integrated functorch into PyTorch. As the final step of the integration, functorch.make_functional_with_buffers is deprecated as of PyTorch 2.0 and will be deleted in a future version of PyTorch >= 2.3. Please use torch.func.functional_call instead; see the PyTorch 2.0 release notes and/or the torch.func migration guide for more details https://pytorch.org/docs/master/func.migrating.html\n",
            "  warn_deprecated('make_functional_with_buffers', 'torch.func.functional_call')\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Labels: \n",
            "tensor([1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0,\n",
            "        0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1,\n",
            "        0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1,\n",
            "        1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0,\n",
            "        1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1,\n",
            "        0, 0, 0, 1, 1, 1, 0, 1])\n",
            "\n",
            "Step: 0\n",
            "\n",
            "Loss: 0.694150984287262\n",
            "\n",
            "\n",
            "\n",
            "****************************************************************************************************\n",
            "\n",
            "\n",
            "Labels: \n",
            "tensor([0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0,\n",
            "        0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0,\n",
            "        1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1,\n",
            "        1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0,\n",
            "        1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0,\n",
            "        1, 0, 1, 1, 1, 0, 0, 0])\n",
            "\n",
            "Step: 1\n",
            "\n",
            "Loss: 1.1528525352478027\n",
            "\n",
            "\n",
            "\n",
            "****************************************************************************************************\n",
            "\n",
            "\n",
            "Labels: \n",
            "tensor([1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1,\n",
            "        1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0,\n",
            "        0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1,\n",
            "        0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0,\n",
            "        0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1,\n",
            "        0, 1, 0, 1, 0, 0, 0, 0])\n",
            "\n",
            "Step: 2\n",
            "\n",
            "Loss: 0.851406455039978\n",
            "\n",
            "\n",
            "\n",
            "****************************************************************************************************\n",
            "\n",
            "\n",
            "Labels: \n",
            "tensor([1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0,\n",
            "        0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1,\n",
            "        1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1,\n",
            "        0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1,\n",
            "        1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1])\n",
            "\n",
            "Step: 3\n",
            "\n",
            "Loss: 6.025542259216309\n",
            "\n",
            "\n",
            "\n",
            "****************************************************************************************************\n",
            "\n",
            "\n",
            "Labels: \n",
            "tensor([1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1,\n",
            "        1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0,\n",
            "        0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1,\n",
            "        1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0,\n",
            "        1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0,\n",
            "        0, 1, 1, 0, 1, 1, 0, 0])\n",
            "\n",
            "Step: 0\n",
            "\n",
            "Loss: 0.6903032660484314\n",
            "\n",
            "\n",
            "\n",
            "****************************************************************************************************\n",
            "\n",
            "\n",
            "Labels: \n",
            "tensor([0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1,\n",
            "        1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1,\n",
            "        0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1,\n",
            "        1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0,\n",
            "        1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0,\n",
            "        0, 1, 1, 0, 1, 0, 1, 0])\n",
            "\n",
            "Step: 1\n",
            "\n",
            "Loss: 0.7938933372497559\n",
            "\n",
            "\n",
            "\n",
            "****************************************************************************************************\n",
            "\n",
            "\n",
            "Labels: \n",
            "tensor([1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1,\n",
            "        1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1,\n",
            "        0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0,\n",
            "        0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0,\n",
            "        0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1,\n",
            "        1, 1, 1, 0, 0, 0, 0, 1])\n",
            "\n",
            "Step: 2\n",
            "\n",
            "Loss: 1.37912118434906\n",
            "\n",
            "\n",
            "\n",
            "****************************************************************************************************\n",
            "\n",
            "\n",
            "Labels: \n",
            "tensor([1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0,\n",
            "        0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0,\n",
            "        0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0,\n",
            "        1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0,\n",
            "        1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1])\n",
            "\n",
            "Step: 3\n",
            "\n",
            "Loss: 1.8350051641464233\n",
            "\n",
            "Epoch: 1\n",
            "Results: \n",
            "{'accuracy': 0.497}\n",
            "\n",
            "Finished Training\n"
          ]
        }
      ],
      "source": [
        "# Train the model\n",
        "classifier.train()\n",
        "for epoch in range(2):\n",
        "    for i, data in enumerate(trainloader, 0):\n",
        "        print(\"\\n\")\n",
        "        print(\"*\"*100)\n",
        "        print(\"\\n\")\n",
        "\n",
        "        if i == 0 and epoch == 0: # Initialize optimizer and model parameters\n",
        "            print(f\"input_ids: {data['input_ids']}\")\n",
        "            print(f\"attention_mask: {data['attention_mask']}\")\n",
        "            print(f\"labels: {data['labels']}\")\n",
        "            # Define optimizer\n",
        "            base_optimizer = torchopt.adam(lr=0.01)\n",
        "            optimizer = fosi_adam_torch(base_optimizer, loss_fn, data, num_iters_to_approx_eigs=500, alpha=0.01)\n",
        "            # func_optimizer = torchopt.FuncOptimizer(fosi_adam_torch(base_optimizer, loss_fn, data, num_iters_to_approx_eigs=500, alpha=0.01))\n",
        "            model, params, buffers = functorch.make_functional_with_buffers(model=classifier)\n",
        "            opt_state = optimizer.init(params)\n",
        "\n",
        "        input_ids = data['input_ids'].squeeze().to(device)\n",
        "        attention_mask = data['attention_mask'].squeeze().to(device)\n",
        "        labels = data['labels'].squeeze().to(device)\n",
        "        print(f\"Labels: \\n{labels}\\n\")\n",
        "\n",
        "        loss = loss_fn(functional_model=model, \n",
        "                       params=params, buffers=buffers, input_ids=input_ids,attention_mask=attention_mask, labels=labels)\n",
        "        print(f\"Step: {i}\\n\")\n",
        "        print(f\"Loss: {loss}\\n\")\n",
        "\n",
        "        # func_optimizer.step(loss, params, inplace=True)\n",
        "\n",
        "        # Calculate gradients\n",
        "        grads = torch.autograd.grad(loss, params)\n",
        "\n",
        "        # Update model parameters\n",
        "        updates, opt_state = optimizer.update(grads, opt_state, params)\n",
        "        params = torchopt.apply_updates(params, updates, inplace=True)\n",
        "\n",
        "        # Should this method exists? Are buffers neseccary to update?\n",
        "        # Update buffers based on updated parameters\n",
        "        # buffers = torchopt.update_buffers(model, params, buffers)\n",
        "    evaluation_results = []\n",
        "\n",
        "    # Evaluate the model\n",
        "    with torch.no_grad():\n",
        "        for i, data in enumerate(testloader):\n",
        "            evaluation_result = {}\n",
        "\n",
        "            input_ids = data['input_ids'].squeeze().to(device)\n",
        "            attention_mask = data['attention_mask'].squeeze().to(device)\n",
        "            labels = data['labels'].squeeze().to(device)\n",
        "\n",
        "            preds = model(params, buffers, input_ids, attention_mask=attention_mask)\n",
        "\n",
        "            predicted_labels = torch.round(preds)\n",
        "\n",
        "            # Save the evaluation results\n",
        "            evaluation_result['input_ids'] = input_ids.cpu().tolist()\n",
        "            evaluation_result['attention_mask'] = attention_mask.cpu().tolist()\n",
        "            evaluation_result['labels'] = labels.cpu().tolist()\n",
        "            evaluation_result['preds'] = preds.cpu().tolist()\n",
        "            evaluation_result['predicted_label'] = predicted_labels.cpu().tolist()\n",
        "\n",
        "            evaluation_results.append(evaluation_result)\n",
        "            metric.add_batch(predictions=predicted_labels, references=labels)\n",
        "\n",
        "print(f'Epoch: {epoch}')\n",
        "results = metric.compute()\n",
        "print(f\"Results: \\n{results}\\n\")\n",
        "\n",
        "print('Finished Training')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1]\n"
          ]
        }
      ],
      "source": [
        "print(evaluation_results[0]['labels'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "dict_keys(['input_ids', 'attention_mask', 'labels', 'preds', 'predicted_label'])\n"
          ]
        }
      ],
      "source": [
        "print(evaluation_results[0].keys())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[[0.32067814469337463], [0.27826035022735596], [0.2859336733818054], [0.38033556938171387], [0.3939129412174225], [0.2985920011997223], [0.40012797713279724], [0.3530847132205963], [0.30622410774230957], [0.3813495934009552], [0.33078932762145996], [0.42219704389572144], [0.34536466002464294], [0.4196912944316864], [0.4567902684211731], [0.2431776076555252], [0.3137512505054474], [0.42258739471435547], [0.3382858335971832], [0.3237937390804291], [0.29999300837516785], [0.35315102338790894], [0.326948344707489], [0.3300442397594452], [0.41341814398765564], [0.39553001523017883], [0.4028570353984833], [0.34860578179359436], [0.2644515335559845], [0.27199968695640564], [0.2460271269083023], [0.33638569712638855], [0.35194525122642517], [0.3956696093082428], [0.28749218583106995], [0.313027560710907], [0.25715571641921997], [0.3575066030025482], [0.4142695963382721], [0.33259639143943787], [0.4077953100204468], [0.43278172612190247], [0.26865363121032715], [0.35169243812561035], [0.41524213552474976], [0.3585192561149597], [0.39150863885879517], [0.3402136266231537], [0.3962009847164154], [0.37759920954704285], [0.31776708364486694], [0.3632301688194275], [0.3735501766204834], [0.3030796945095062], [0.24741996824741364], [0.33053508400917053], [0.3097030818462372], [0.2869977355003357], [0.34602251648902893], [0.3660905659198761], [0.3872086703777313], [0.42390552163124084], [0.30660441517829895], [0.3361411392688751], [0.4085692763328552], [0.3914243280887604], [0.30887696146965027], [0.29482701420783997], [0.33743759989738464], [0.32646679878234863], [0.36239275336265564], [0.32305875420570374], [0.39486002922058105], [0.3401551842689514], [0.39067456126213074], [0.36561402678489685], [0.29193827509880066], [0.3173674941062927], [0.28972846269607544], [0.318109393119812], [0.35933151841163635], [0.2457825094461441], [0.3867817521095276], [0.3280501663684845], [0.3168991506099701], [0.37066254019737244], [0.30803540349006653], [0.2877728044986725], [0.38520392775535583], [0.3627903461456299], [0.35828158259391785], [0.37969258427619934], [0.3443123996257782], [0.327809602022171], [0.2615308463573456], [0.38249945640563965], [0.2928580343723297], [0.39304402470588684], [0.3801209628582001], [0.3107379376888275], [0.447163850069046], [0.3236877918243408], [0.24059449136257172], [0.43989983201026917], [0.36359402537345886], [0.3250301778316498], [0.3385589122772217], [0.3118438720703125], [0.23993951082229614], [0.3307512104511261], [0.32560011744499207], [0.28329992294311523], [0.3859576880931854], [0.3222459554672241], [0.37468796968460083], [0.372109591960907], [0.32950255274772644], [0.4430861473083496], [0.3170031011104584], [0.30170461535453796], [0.25461140275001526], [0.31149476766586304], [0.3428637981414795], [0.327510803937912], [0.3218390643596649], [0.41325637698173523], [0.39357611536979675], [0.3848540782928467]]\n"
          ]
        }
      ],
      "source": [
        "print(evaluation_results[0]['preds'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[[0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0]]\n"
          ]
        }
      ],
      "source": [
        "print(evaluation_results[0]['predicted_label'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "id": "o53j7gDuFdBB"
      },
      "outputs": [],
      "source": [
        "import gc\n",
        "gc.collect() # Python thing\n",
        "torch.cuda.empty_cache() # PyTorch thing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "G0N3gFB0FvhT"
      },
      "outputs": [],
      "source": [
        "# # Train the model\n",
        "# for epoch in range(3):\n",
        "#     for i, data in enumerate(trainloader, 0):\n",
        "#         print(\"\\n\")\n",
        "#         print(\"*\"*100)\n",
        "#         print(\"\\n\")\n",
        "\n",
        "#         if i == 0: # Initialize optimizer and model parameters\n",
        "#             print(f\"input_ids: {data['input_ids']}\")\n",
        "#             print(f\"attention_mask: {data['attention_mask']}\")\n",
        "#             print(f\"labels: {data['labels']}\")\n",
        "#             # Define optimizer\n",
        "#             base_optimizer = torchopt.adam(lr=0.01)\n",
        "#             optimizer = fosi_adam_torch(base_optimizer, loss_fn, data, num_iters_to_approx_eigs=500, alpha=0.01)\n",
        "#             model, params, buffers = functorch.make_functional_with_buffers(model=base_model)\n",
        "#             # model, params = functorch.make_functional(model=base_model)\n",
        "#             opt_state = optimizer.init(params)\n",
        "#             # model.train()\n",
        "\n",
        "\n",
        "#         input_ids = data['input_ids']\n",
        "#         attention_mask = data['attention_mask']\n",
        "#         labels = data['labels']\n",
        "#         print(f\"Labels: \\n{labels}\\n\")\n",
        "\n",
        "#         loss = loss_fn(params, buffers, input_ids, attention_mask, labels)\n",
        "#         # loss = loss_fn(params, input_ids, attention_mask, labels)\n",
        "#         print(f\"Step: {i}\\n\")\n",
        "#         print(f\"Loss: {loss}\\n\")\n",
        "\n",
        "#         print(f\"Calculating Gradients\\n\")\n",
        "#         grads = torch.autograd.grad(loss, params)\n",
        "#         # print(f\"Grads: \\n{grads}\\n\")\n",
        "#         print(f\"Calculating updates in the model...\\n\")\n",
        "#         updates, opt_state = optimizer.update(grads, opt_state, params)\n",
        "#         print(\"Applying updates\\n\")\n",
        "#         params = torchopt.apply_updates(params, updates, inplace=True)\n",
        "\n",
        "#     # Evaluate the model\n",
        "#     # model.eval()\n",
        "#     with torch.no_grad():\n",
        "#         for data in testloader:\n",
        "#             input_ids = data['input_ids'].to(device)\n",
        "#             attention_mask = data['attention_mask'].to(device)\n",
        "#             labels = data['labels'].to(device)\n",
        "\n",
        "#             logits = model(params, buffers, input_ids, attention_mask=attention_mask).logits\n",
        "#             # logits = model(params, input_ids, attention_mask=attention_mask).logits\n",
        "#             print(f\"Logits: {logits}\")\n",
        "#             logits_tensor = torch.tensor(logits)\n",
        "#             softmax_output = F.softmax(logits_tensor, dim=1)\n",
        "\n",
        "#             print(f\"SoftMax of Logits: {softmax_output}\")\n",
        "#             predicted_labels = torch.argmax(softmax_output, dim=1)\n",
        "#             print(f\"Argmax Predictions: {predicted_labels}\")\n",
        "\n",
        "#             metric.add_batch(predictions=predicted_labels, references=labels)\n",
        "\n",
        "\n",
        "#     print(f'Epoch: {epoch}')\n",
        "#     results = metric.compute()\n",
        "#     print(f\"Results: \\n{results}\\n\")\n",
        "#     # print(f'Correct: {correct}\\n')\n",
        "#     # print(f'Total: {total}\\n')\n",
        "#     # print(f'Predicted: {predicted}\\n')\n",
        "#     # print(f'outputs: {outputs}\\n')\n",
        "#     # print(f'labels: {labels}\\n')\n",
        "#     # print(f'Accuracy of the network on the test data: {100 * correct / total}%')\n",
        "#     # model.train() # for the next epoch\n",
        "\n",
        "# print('Finished Training')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NYI0DC4MlAnm"
      },
      "outputs": [],
      "source": [
        "# # Evaluate the model\n",
        "# model.eval()\n",
        "# correct = 0\n",
        "# total = 0\n",
        "# with torch.no_grad():\n",
        "#     for data in testloader:\n",
        "#         input_ids = data['input_ids'].to(device)\n",
        "#         attention_mask = data['attention_mask'].to(device)\n",
        "#         labels = data['labels'].to(device)\n",
        "#         outputs = model(params, buffers, input_ids, attention_mask=attention_mask)\n",
        "#         _, predicted = torch.max(outputs.logits, 1)\n",
        "#         total += labels.size(0)\n",
        "#         correct += (predicted == labels).sum().item()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Itjc8AT9FvhU"
      },
      "outputs": [],
      "source": [
        "# # Define training loop\n",
        "# for epoch in range(3):  # Adjust number of epochs as needed\n",
        "#     model.train()\n",
        "#     for i, batch in enumerate(train_dataset):\n",
        "#         input_ids = batch['input_ids']\n",
        "#         attention_mask = batch['attention_mask']\n",
        "#         labels = batch['label']\n",
        "\n",
        "#         #This is taking care automatically\n",
        "#         # optimizer.zero_grad() This is taking care automatically\n",
        "\n",
        "#         outputs = model(input_ids=input_ids, attention_mask=attention_mask, labels=labels)\n",
        "#         loss = outputs.loss\n",
        "#         loss.backward()\n",
        "#         optimizer.step()\n",
        "\n",
        "#         # if (i + 1) % 100 == 0:\n",
        "#         #     print(f'[{epoch + 1}, {i + 1:5d}] loss: {loss.item():.3f}')\n",
        "\n",
        "#     # Evaluate on test set\n",
        "#     model.eval()\n",
        "#     total = 0\n",
        "#     correct = 0\n",
        "#     with torch.no_grad():\n",
        "#         for batch in test_dataset:\n",
        "#             input_ids = batch['input_ids']\n",
        "#             attention_mask = batch['attention_mask']\n",
        "#             labels = batch['label']\n",
        "#             print(\"Labels: \\n\", labels)\n",
        "\n",
        "#             outputs = model(input_ids, attention_mask=attention_mask, labels=labels)\n",
        "#             logits = outputs.logits\n",
        "#             _, predicted = torch.max(logits, 1)\n",
        "#             total += labels.size(0)\n",
        "#             correct += (predicted == labels).sum().item()\n",
        "\n",
        "#     accuracy = correct / total\n",
        "#     print(f'Test accuracy: {accuracy:.4f}')\n",
        "\n",
        "# print('Finished Training')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VYHLY-DRFvhU"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "include_colab_link": true,
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.2"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
