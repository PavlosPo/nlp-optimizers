{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "view-in-github"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/PavlosPo/nlp-optimizers/blob/pavlos-playground/pytorch-experiments-1-fosi-adam/playground.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JVmTDAZPH2-f"
      },
      "source": [
        "## FOSI Classes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 46,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pD-T81dbHzYD",
        "outputId": "f92a1b0f-2b0b-4983-ae7c-cbee3b1ff7ff"
      },
      "outputs": [],
      "source": [
        "# ! rm -rf ./fosi/\n",
        "# !mkdir ./fosi/\n",
        "# !unzip fosi.zip -d ./fosi/"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZYKw68tXH0Ue"
      },
      "source": [
        "## Working Example"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 47,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZtEnvL7sHwCC",
        "outputId": "27e6e68c-4c93-49e5-e04c-676253a35000"
      },
      "outputs": [],
      "source": [
        "# !pip install torchopt\n",
        "# !pip install datasets\n",
        "# !pip install evaluate"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 48,
      "metadata": {
        "id": "VVIyQm1XFvhQ"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torchvision.transforms as transforms\n",
        "from torch.utils.data.dataloader import default_collate\n",
        "from transformers import AutoModelForSequenceClassification, AutoTokenizer, DataCollatorWithPadding, Trainer, TrainingArguments\n",
        "from torch.optim import Adam\n",
        "import torch.nn as nn\n",
        "import torchopt\n",
        "import functorch\n",
        "import evaluate\n",
        "import torch.nn.functional as F\n",
        "\n",
        "from datasets import load_dataset\n",
        "from fosi import fosi_adam_torch\n",
        "\n",
        "# # Set device\n",
        "# device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
        "\n",
        "# # Load pre-trained DistilBERT model and tokenizer\n",
        "# base_model = AutoModelForSequenceClassification.from_pretrained('bert-base-uncased', num_labels=2)\n",
        "# base_model.to(device)\n",
        "# tokenizer = AutoTokenizer.from_pretrained('bert-base-uncased')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 49,
      "metadata": {
        "id": "vB2RtawpGA8R"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "BertClassifier(\n",
              "  (bert): BertForSequenceClassification(\n",
              "    (bert): BertModel(\n",
              "      (embeddings): BertEmbeddings(\n",
              "        (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
              "        (position_embeddings): Embedding(512, 768)\n",
              "        (token_type_embeddings): Embedding(2, 768)\n",
              "        (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "        (dropout): Dropout(p=0.1, inplace=False)\n",
              "      )\n",
              "      (encoder): BertEncoder(\n",
              "        (layer): ModuleList(\n",
              "          (0-11): 12 x BertLayer(\n",
              "            (attention): BertAttention(\n",
              "              (self): BertSelfAttention(\n",
              "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "                (dropout): Dropout(p=0.1, inplace=False)\n",
              "              )\n",
              "              (output): BertSelfOutput(\n",
              "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "                (dropout): Dropout(p=0.1, inplace=False)\n",
              "              )\n",
              "            )\n",
              "            (intermediate): BertIntermediate(\n",
              "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "              (intermediate_act_fn): GELUActivation()\n",
              "            )\n",
              "            (output): BertOutput(\n",
              "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "        )\n",
              "      )\n",
              "      (pooler): BertPooler(\n",
              "        (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "        (activation): Tanh()\n",
              "      )\n",
              "    )\n",
              "    (dropout): Dropout(p=0.1, inplace=False)\n",
              "    (classifier): Linear(in_features=768, out_features=1, bias=True)\n",
              "  )\n",
              "  (sigmoid): Sigmoid()\n",
              ")"
            ]
          },
          "execution_count": 49,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from transformers import AutoModelForSequenceClassification, AutoTokenizer, DataCollatorWithPadding\n",
        "\n",
        "# Load pre-trained BERT model and tokenizer\n",
        "bert_model_name = \"bert-base-uncased\"\n",
        "num_classes = 1\n",
        "bert_model = AutoModelForSequenceClassification.from_pretrained(bert_model_name, num_labels=num_classes)\n",
        "tokenizer = AutoTokenizer.from_pretrained(bert_model_name)\n",
        "\n",
        "# # Freeze all parameters of the BERT model except for the last layer (classifier)\n",
        "# for name, param in bert_model.named_parameters():\n",
        "#     if 'classifier' not in name:  # Exclude parameters of the classifier layer\n",
        "#         param.requires_grad = False\n",
        "\n",
        "# Define a dense layer on top of the BERT model for classification\n",
        "class BertClassifier(nn.Module):\n",
        "    def __init__(self, bert_model, num_classes):\n",
        "        super(BertClassifier, self).__init__()\n",
        "        self.bert = bert_model\n",
        "        self.sigmoid = nn.Sigmoid()\n",
        "\n",
        "    def forward(self, input_ids, attention_mask):\n",
        "        outputs = self.bert(input_ids=input_ids, attention_mask=attention_mask).logits\n",
        "        probability = self.sigmoid(outputs.squeeze())\n",
        "        return probability\n",
        "\n",
        "# Instantiate the classifier\n",
        "classifier = BertClassifier(bert_model, num_classes)\n",
        "\n",
        "# Optionally, move the model to a GPU device if available\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "classifier.to(device)\n",
        "\n",
        "# Now you can train the entire model (BERT + dense layer) on your task-specific data\n",
        "# Make sure to prepare your data (input_ids, attention_mask, labels) using the BERT tokenizer\n",
        "# and use an appropriate loss function and optimizer for your task\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 50,
      "metadata": {
        "id": "NEhHBnRiFvhR"
      },
      "outputs": [],
      "source": [
        "\n",
        "# Define a function to preprocess the dataset\n",
        "def prepare_dataset(example):\n",
        "    return tokenizer(example['sentence'], add_special_tokens=True, truncation=True, padding=True, return_tensors='pt')\n",
        "\n",
        "data_collator = DataCollatorWithPadding(tokenizer=tokenizer, padding=True)\n",
        "\n",
        "dataset = load_dataset('glue', 'sst2').map(prepare_dataset, batched=True)\n",
        "metric = evaluate.load(\"glue\", \"sst2\")\n",
        "\n",
        "# Split dataset into train and test sets, we use the train category because the test one has labels -1 only.\n",
        "train_dataset = dataset['train'].select(range(0,500)).remove_columns(['sentence', 'idx']).rename_column('label', 'labels')\n",
        "test_dataset = dataset['train'].select(range(500, 1000)).remove_columns(['sentence', 'idx']).rename_column('label', 'labels')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 51,
      "metadata": {
        "id": "7zU_KRNimwR1"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "67349"
            ]
          },
          "execution_count": 51,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "len(dataset['train']['label'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 52,
      "metadata": {
        "id": "KaC8HrwiFvhS"
      },
      "outputs": [],
      "source": [
        "# Define data loaders\n",
        "batch_size = 128\n",
        "trainloader = torch.utils.data.DataLoader(train_dataset, batch_size=batch_size, shuffle=True, collate_fn=data_collator)\n",
        "testloader = torch.utils.data.DataLoader(test_dataset, batch_size=batch_size, shuffle=True, collate_fn=data_collator)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 53,
      "metadata": {
        "id": "ikDlQ1UDJYGT"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor([[  101,  2130, 25591,  ...,     0,     0,     0],\n",
            "        [  101,  2003,  1037,  ...,     0,     0,     0],\n",
            "        [  101,  2437, 10556,  ...,     0,     0,     0],\n",
            "        ...,\n",
            "        [  101,  8562,  3238,  ...,     0,     0,     0],\n",
            "        [  101,  2563,  1005,  ...,     0,     0,     0],\n",
            "        [  101,  2009,  5078,  ...,     0,     0,     0]])\n"
          ]
        }
      ],
      "source": [
        "for i in (testloader):\n",
        "  print(i['input_ids'])\n",
        "  break"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 54,
      "metadata": {
        "id": "TO8H7XfbFvhT"
      },
      "outputs": [],
      "source": [
        "# With buffers\n",
        "\n",
        "def loss_fn(functional_model, params, buffers, input_ids, attention_mask, labels):\n",
        "    preds = functional_model(params, buffers=buffers, input_ids=input_ids, attention_mask=attention_mask)\n",
        "    loss = nn.functional.binary_cross_entropy(preds.squeeze().to(torch.float32), labels.squeeze().to(torch.float32))\n",
        "    return loss\n",
        "\n",
        "# def softmax_output_fn(params, buffers, input_ids, attention_mask, labels):\n",
        "#     logits = model(params, buffers=buffers, input_ids=input_ids, attention_mask=attention_mask, labels=labels).logits\n",
        "#     logits_tensor = torch.tensor(logits)\n",
        "#     softmax_output = F.softmax(logits_tensor, dim=1)\n",
        "#     return softmax_output"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 55,
      "metadata": {
        "id": "sRrhbO-bIJDN"
      },
      "outputs": [],
      "source": [
        "# Without buffers\n",
        "\n",
        "# def loss_fn(params, input_ids, attention_mask, labels):\n",
        "#     loss = model(params, input_ids=input_ids, attention_mask=attention_mask, labels=labels).loss\n",
        "#     return loss\n",
        "\n",
        "# def softmax_output_fn(params, input_ids, attention_mask, labels):\n",
        "#     logits = model(params, input_ids=input_ids, attention_mask=attention_mask, labels=labels).logits\n",
        "#     logits_tensor = torch.tensor(logits)\n",
        "#     softmax_output = F.softmax(logits_tensor, dim=1)\n",
        "#     return softmax_output"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 66,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "input_ids: tensor([[  101, 16514,  2135,  ...,     0,     0,     0],\n",
            "        [  101,  2053,  2047,  ...,     0,     0,     0],\n",
            "        [  101,  2521, 24763,  ...,     0,     0,     0],\n",
            "        ...,\n",
            "        [  101,  2009, 11014,  ...,     0,     0,     0],\n",
            "        [  101,  2003,  3432,  ...,     0,     0,     0],\n",
            "        [  101,  2202,  2729,  ...,     0,     0,     0]])\n",
            "attention_mask: tensor([[1, 1, 1,  ..., 0, 0, 0],\n",
            "        [1, 1, 1,  ..., 0, 0, 0],\n",
            "        [1, 1, 1,  ..., 0, 0, 0],\n",
            "        ...,\n",
            "        [1, 1, 1,  ..., 0, 0, 0],\n",
            "        [1, 1, 1,  ..., 0, 0, 0],\n",
            "        [1, 1, 1,  ..., 0, 0, 0]])\n",
            "labels: tensor([1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "        1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1,\n",
            "        0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0,\n",
            "        0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1,\n",
            "        1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0,\n",
            "        0, 1, 0, 0, 1, 0, 0, 1])\n",
            "Returned ESE function. Lanczos order (m) is 20 .\n"
          ]
        }
      ],
      "source": [
        "\n",
        "# Initialize optimizer and model parameters\n",
        "# Those are needed in order for fosi_adam_to run in the loop below\n",
        "\n",
        "data = next(iter(trainloader))  # get first batch of data\n",
        "\n",
        "print(f\"input_ids: {data['input_ids']}\")\n",
        "print(f\"attention_mask: {data['attention_mask']}\")\n",
        "print(f\"labels: {data['labels']}\")\n",
        "\n",
        "# Define optimizer\n",
        "classifier.train()\n",
        "base_optimizer = torchopt.adam(lr=0.01)\n",
        "optimizer = fosi_adam_torch(base_optimizer, loss_fn, data, num_iters_to_approx_eigs=500, alpha=0.01)\n",
        "model, params, buffers = functorch.make_functional_with_buffers(model=classifier)\n",
        "opt_state = optimizer.init(params)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 67,
      "metadata": {
        "id": "QU6tGV1toSnb"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "****************************************************************************************************\n",
            "\n",
            "\n",
            "Labels: \n",
            "tensor([0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0,\n",
            "        0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0,\n",
            "        0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1,\n",
            "        0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0,\n",
            "        0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1,\n",
            "        1, 1, 1, 0, 0, 1, 1, 1])\n",
            "\n",
            "Step: 1\n",
            "\n",
            "Loss: 0.6880813837051392\n",
            "\n",
            "\n",
            "\n",
            "****************************************************************************************************\n",
            "\n",
            "\n",
            "Labels: \n",
            "tensor([0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1,\n",
            "        1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1,\n",
            "        0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1,\n",
            "        0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0,\n",
            "        1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1,\n",
            "        1, 0, 0, 1, 0, 1, 1, 0])\n",
            "\n",
            "Step: 2\n",
            "\n",
            "Loss: 2.0048704147338867\n",
            "\n",
            "\n",
            "\n",
            "****************************************************************************************************\n",
            "\n",
            "\n",
            "Labels: \n",
            "tensor([1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1,\n",
            "        0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1,\n",
            "        1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0,\n",
            "        1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "        0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0,\n",
            "        1, 0, 0, 1, 1, 1, 0, 1])\n",
            "\n",
            "Step: 3\n",
            "\n",
            "Loss: 2.6676766872406006\n",
            "\n",
            "\n",
            "\n",
            "****************************************************************************************************\n",
            "\n",
            "\n",
            "Labels: \n",
            "tensor([1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0,\n",
            "        0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0,\n",
            "        0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1,\n",
            "        0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0,\n",
            "        1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1])\n",
            "\n",
            "Step: 4\n",
            "\n",
            "Loss: 2.4488956928253174\n",
            "\n",
            "\n",
            "\n",
            "****************************************************************************************************\n",
            "\n",
            "\n",
            "Labels: \n",
            "tensor([1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0,\n",
            "        0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0,\n",
            "        1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0,\n",
            "        0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1,\n",
            "        0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0,\n",
            "        1, 0, 1, 0, 1, 1, 1, 0])\n",
            "\n",
            "Step: 1\n",
            "\n",
            "Loss: 2.3199398517608643\n",
            "\n",
            "\n",
            "\n",
            "****************************************************************************************************\n",
            "\n",
            "\n",
            "Labels: \n",
            "tensor([1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0,\n",
            "        1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1,\n",
            "        1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1,\n",
            "        1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0,\n",
            "        1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0,\n",
            "        1, 1, 1, 1, 1, 1, 0, 1])\n",
            "\n",
            "Step: 2\n",
            "\n",
            "Loss: 1.5729990005493164\n",
            "\n",
            "\n",
            "\n",
            "****************************************************************************************************\n",
            "\n",
            "\n",
            "Labels: \n",
            "tensor([1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1,\n",
            "        1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1,\n",
            "        0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1,\n",
            "        1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1,\n",
            "        1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0,\n",
            "        1, 1, 1, 0, 1, 0, 1, 1])\n",
            "\n",
            "Step: 3\n",
            "\n",
            "Loss: 0.8320564031600952\n",
            "\n",
            "\n",
            "\n",
            "****************************************************************************************************\n",
            "\n",
            "\n",
            "Labels: \n",
            "tensor([0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1,\n",
            "        0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0,\n",
            "        0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1,\n",
            "        0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1,\n",
            "        1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1])\n",
            "\n",
            "Step: 4\n",
            "\n",
            "Loss: 0.7185982465744019\n",
            "\n",
            "Epoch: 1\n",
            "Results: \n",
            "{'accuracy': 0.502}\n",
            "\n",
            "Finished Training\n"
          ]
        }
      ],
      "source": [
        "# Train the model\n",
        "for epoch in range(2):\n",
        "    for i, data in enumerate(trainloader, 1):\n",
        "        print(\"\\n\")\n",
        "        print(\"*\"*100)\n",
        "        print(\"\\n\")\n",
        "\n",
        "        input_ids = data['input_ids'].squeeze().to(device)\n",
        "        attention_mask = data['attention_mask'].squeeze().to(device)\n",
        "        labels = data['labels'].squeeze().to(device)\n",
        "        print(f\"Labels: \\n{labels}\\n\")\n",
        "\n",
        "        loss = loss_fn(functional_model=model, \n",
        "                       params=params, buffers=buffers, input_ids=input_ids,attention_mask=attention_mask, labels=labels)\n",
        "        print(f\"Step: {i}\\n\")\n",
        "        print(f\"Loss: {loss}\\n\")\n",
        "\n",
        "        # func_optimizer.step(loss, params, inplace=True)\n",
        "\n",
        "        # Calculate gradients\n",
        "        grads = torch.autograd.grad(loss, params)\n",
        "\n",
        "        # Update model parameters\n",
        "        updates, opt_state = optimizer.update(grads, opt_state, params)\n",
        "        params = torchopt.apply_updates(params, updates, inplace=True)\n",
        "\n",
        "        # Should this method exists? Are buffers neseccary to update?\n",
        "        # Update buffers based on updated parameters\n",
        "        # buffers = torchopt.update_buffers(model, params, buffers)\n",
        "    evaluation_results = []\n",
        "\n",
        "    # Evaluate the model\n",
        "    classifier.eval()\n",
        "    with torch.no_grad():\n",
        "        for i, data in enumerate(testloader):\n",
        "            evaluation_result = {}\n",
        "\n",
        "            input_ids = data['input_ids'].squeeze().to(device)\n",
        "            attention_mask = data['attention_mask'].squeeze().to(device)\n",
        "            labels = data['labels'].squeeze().to(device)\n",
        "\n",
        "            preds = model(params, buffers, input_ids, attention_mask=attention_mask)\n",
        "\n",
        "            predicted_labels = torch.round(preds)\n",
        "\n",
        "            # Save the evaluation results\n",
        "            evaluation_result['input_ids'] = input_ids.cpu().tolist()\n",
        "            evaluation_result['attention_mask'] = attention_mask.cpu().tolist()\n",
        "            evaluation_result['labels'] = labels.cpu().tolist()\n",
        "            evaluation_result['preds'] = preds.cpu().tolist()\n",
        "            evaluation_result['predicted_label'] = predicted_labels.cpu().tolist()\n",
        "\n",
        "            evaluation_results.append(evaluation_result)\n",
        "            metric.add_batch(predictions=predicted_labels, references=labels)\n",
        "\n",
        "print(f'Epoch: {epoch}')\n",
        "results = metric.compute()\n",
        "print(f\"Results: \\n{results}\\n\")\n",
        "\n",
        "print('Finished Training')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 68,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1]\n"
          ]
        }
      ],
      "source": [
        "print(evaluation_results[0]['labels'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 69,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "dict_keys(['input_ids', 'attention_mask', 'labels', 'preds', 'predicted_label'])\n"
          ]
        }
      ],
      "source": [
        "print(evaluation_results[0].keys())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 70,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[0.36563795804977417, 0.3529967665672302, 0.3821791708469391, 0.4057554602622986, 0.3534340560436249, 0.3860921263694763, 0.31445321440696716, 0.35191407799720764, 0.36207184195518494, 0.4616522789001465, 0.46333950757980347, 0.40192654728889465, 0.3814569115638733, 0.3836885094642639, 0.31321561336517334, 0.4732728600502014, 0.5138583183288574, 0.37511923909187317, 0.47060903906822205, 0.3644796907901764, 0.31288063526153564, 0.3747831881046295, 0.34744465351104736, 0.4050697088241577, 0.45658013224601746, 0.35638412833213806, 0.406047523021698, 0.46350979804992676, 0.372139573097229, 0.42184123396873474, 0.4497975707054138, 0.3508889675140381, 0.29819008708000183, 0.37678325176239014, 0.31483420729637146, 0.40384426712989807, 0.4338630139827728, 0.3745567202568054, 0.41909345984458923, 0.41355180740356445, 0.4067532420158386, 0.35739511251449585, 0.36872386932373047, 0.3335361182689667, 0.36576971411705017, 0.4096563160419464, 0.350468248128891, 0.35859355330467224, 0.3648141920566559, 0.41268739104270935, 0.4185018241405487, 0.4095582365989685, 0.37309569120407104, 0.3670097887516022, 0.3236525356769562, 0.4031662046909332, 0.35950982570648193, 0.4000932574272156, 0.4235616624355316, 0.37212419509887695, 0.3645835518836975, 0.3808145225048065, 0.3857874274253845, 0.3059383034706116, 0.41145947575569153, 0.3623303771018982, 0.4153159558773041, 0.4285665452480316, 0.3906351923942566, 0.33803561329841614, 0.42254453897476196, 0.34802165627479553, 0.31898587942123413, 0.41277992725372314, 0.40213683247566223, 0.3144535422325134, 0.41054680943489075, 0.3051717281341553, 0.3548959195613861, 0.2897627055644989, 0.3426392078399658, 0.49030163884162903, 0.3211636543273926, 0.3545083701610565, 0.3853112757205963, 0.32359978556632996, 0.3997664451599121, 0.42738083004951477, 0.288419634103775, 0.3226865530014038, 0.34397321939468384, 0.4559168815612793, 0.3690733313560486, 0.33472543954849243, 0.40756845474243164, 0.4686710834503174, 0.3610045611858368, 0.35673657059669495, 0.37296658754348755, 0.3409428298473358, 0.4506520628929138, 0.45902255177497864, 0.4131799638271332, 0.3340245485305786, 0.3992566168308258, 0.42733755707740784, 0.3876802325248718, 0.4457162618637085, 0.30465981364250183, 0.46967071294784546, 0.3659330904483795, 0.47378286719322205, 0.3522934913635254, 0.4873462915420532, 0.43860021233558655, 0.38622137904167175, 0.3534573018550873, 0.320986270904541, 0.41866356134414673, 0.2963941991329193, 0.41571044921875, 0.37616845965385437, 0.3300215005874634, 0.4215650260448456, 0.35910406708717346, 0.3624330163002014, 0.37562498450279236, 0.3887065649032593]\n"
          ]
        }
      ],
      "source": [
        "print(evaluation_results[0]['preds'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 71,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]\n"
          ]
        }
      ],
      "source": [
        "print(evaluation_results[0]['predicted_label'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 62,
      "metadata": {
        "id": "o53j7gDuFdBB"
      },
      "outputs": [],
      "source": [
        "import gc\n",
        "gc.collect() # Python thing\n",
        "torch.cuda.empty_cache() # PyTorch thing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 63,
      "metadata": {
        "id": "G0N3gFB0FvhT"
      },
      "outputs": [],
      "source": [
        "# # Train the model\n",
        "# for epoch in range(3):\n",
        "#     for i, data in enumerate(trainloader, 0):\n",
        "#         print(\"\\n\")\n",
        "#         print(\"*\"*100)\n",
        "#         print(\"\\n\")\n",
        "\n",
        "#         if i == 0: # Initialize optimizer and model parameters\n",
        "#             print(f\"input_ids: {data['input_ids']}\")\n",
        "#             print(f\"attention_mask: {data['attention_mask']}\")\n",
        "#             print(f\"labels: {data['labels']}\")\n",
        "#             # Define optimizer\n",
        "#             base_optimizer = torchopt.adam(lr=0.01)\n",
        "#             optimizer = fosi_adam_torch(base_optimizer, loss_fn, data, num_iters_to_approx_eigs=500, alpha=0.01)\n",
        "#             model, params, buffers = functorch.make_functional_with_buffers(model=base_model)\n",
        "#             # model, params = functorch.make_functional(model=base_model)\n",
        "#             opt_state = optimizer.init(params)\n",
        "#             # model.train()\n",
        "\n",
        "\n",
        "#         input_ids = data['input_ids']\n",
        "#         attention_mask = data['attention_mask']\n",
        "#         labels = data['labels']\n",
        "#         print(f\"Labels: \\n{labels}\\n\")\n",
        "\n",
        "#         loss = loss_fn(params, buffers, input_ids, attention_mask, labels)\n",
        "#         # loss = loss_fn(params, input_ids, attention_mask, labels)\n",
        "#         print(f\"Step: {i}\\n\")\n",
        "#         print(f\"Loss: {loss}\\n\")\n",
        "\n",
        "#         print(f\"Calculating Gradients\\n\")\n",
        "#         grads = torch.autograd.grad(loss, params)\n",
        "#         # print(f\"Grads: \\n{grads}\\n\")\n",
        "#         print(f\"Calculating updates in the model...\\n\")\n",
        "#         updates, opt_state = optimizer.update(grads, opt_state, params)\n",
        "#         print(\"Applying updates\\n\")\n",
        "#         params = torchopt.apply_updates(params, updates, inplace=True)\n",
        "\n",
        "#     # Evaluate the model\n",
        "#     # model.eval()\n",
        "#     with torch.no_grad():\n",
        "#         for data in testloader:\n",
        "#             input_ids = data['input_ids'].to(device)\n",
        "#             attention_mask = data['attention_mask'].to(device)\n",
        "#             labels = data['labels'].to(device)\n",
        "\n",
        "#             logits = model(params, buffers, input_ids, attention_mask=attention_mask).logits\n",
        "#             # logits = model(params, input_ids, attention_mask=attention_mask).logits\n",
        "#             print(f\"Logits: {logits}\")\n",
        "#             logits_tensor = torch.tensor(logits)\n",
        "#             softmax_output = F.softmax(logits_tensor, dim=1)\n",
        "\n",
        "#             print(f\"SoftMax of Logits: {softmax_output}\")\n",
        "#             predicted_labels = torch.argmax(softmax_output, dim=1)\n",
        "#             print(f\"Argmax Predictions: {predicted_labels}\")\n",
        "\n",
        "#             metric.add_batch(predictions=predicted_labels, references=labels)\n",
        "\n",
        "\n",
        "#     print(f'Epoch: {epoch}')\n",
        "#     results = metric.compute()\n",
        "#     print(f\"Results: \\n{results}\\n\")\n",
        "#     # print(f'Correct: {correct}\\n')\n",
        "#     # print(f'Total: {total}\\n')\n",
        "#     # print(f'Predicted: {predicted}\\n')\n",
        "#     # print(f'outputs: {outputs}\\n')\n",
        "#     # print(f'labels: {labels}\\n')\n",
        "#     # print(f'Accuracy of the network on the test data: {100 * correct / total}%')\n",
        "#     # model.train() # for the next epoch\n",
        "\n",
        "# print('Finished Training')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 64,
      "metadata": {
        "id": "NYI0DC4MlAnm"
      },
      "outputs": [],
      "source": [
        "# # Evaluate the model\n",
        "# model.eval()\n",
        "# correct = 0\n",
        "# total = 0\n",
        "# with torch.no_grad():\n",
        "#     for data in testloader:\n",
        "#         input_ids = data['input_ids'].to(device)\n",
        "#         attention_mask = data['attention_mask'].to(device)\n",
        "#         labels = data['labels'].to(device)\n",
        "#         outputs = model(params, buffers, input_ids, attention_mask=attention_mask)\n",
        "#         _, predicted = torch.max(outputs.logits, 1)\n",
        "#         total += labels.size(0)\n",
        "#         correct += (predicted == labels).sum().item()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 65,
      "metadata": {
        "id": "Itjc8AT9FvhU"
      },
      "outputs": [],
      "source": [
        "# # Define training loop\n",
        "# for epoch in range(3):  # Adjust number of epochs as needed\n",
        "#     model.train()\n",
        "#     for i, batch in enumerate(train_dataset):\n",
        "#         input_ids = batch['input_ids']\n",
        "#         attention_mask = batch['attention_mask']\n",
        "#         labels = batch['label']\n",
        "\n",
        "#         #This is taking care automatically\n",
        "#         # optimizer.zero_grad() This is taking care automatically\n",
        "\n",
        "#         outputs = model(input_ids=input_ids, attention_mask=attention_mask, labels=labels)\n",
        "#         loss = outputs.loss\n",
        "#         loss.backward()\n",
        "#         optimizer.step()\n",
        "\n",
        "#         # if (i + 1) % 100 == 0:\n",
        "#         #     print(f'[{epoch + 1}, {i + 1:5d}] loss: {loss.item():.3f}')\n",
        "\n",
        "#     # Evaluate on test set\n",
        "#     model.eval()\n",
        "#     total = 0\n",
        "#     correct = 0\n",
        "#     with torch.no_grad():\n",
        "#         for batch in test_dataset:\n",
        "#             input_ids = batch['input_ids']\n",
        "#             attention_mask = batch['attention_mask']\n",
        "#             labels = batch['label']\n",
        "#             print(\"Labels: \\n\", labels)\n",
        "\n",
        "#             outputs = model(input_ids, attention_mask=attention_mask, labels=labels)\n",
        "#             logits = outputs.logits\n",
        "#             _, predicted = torch.max(logits, 1)\n",
        "#             total += labels.size(0)\n",
        "#             correct += (predicted == labels).sum().item()\n",
        "\n",
        "#     accuracy = correct / total\n",
        "#     print(f'Test accuracy: {accuracy:.4f}')\n",
        "\n",
        "# print('Finished Training')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VYHLY-DRFvhU"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "include_colab_link": true,
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.2"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
