{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "view-in-github"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/PavlosPo/nlp-optimizers/blob/pavlos-playground/pytorch-experiments-1-fosi-adam/playground.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JVmTDAZPH2-f"
      },
      "source": [
        "## FOSI Classes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pD-T81dbHzYD",
        "outputId": "f92a1b0f-2b0b-4983-ae7c-cbee3b1ff7ff"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Archive:  fosi.zip\n",
            " extracting: ./fosi/__init__.py      \n",
            "   creating: ./fosi/jax_optim/\n",
            " extracting: ./fosi/jax_optim/__init__.py  \n",
            " extracting: ./fosi/jax_optim/extreme_spectrum_estimation.py  \n",
            " extracting: ./fosi/jax_optim/fosi_optimizer.py  \n",
            " extracting: ./fosi/jax_optim/lanczos_algorithm.py  \n",
            " extracting: ./fosi/jax_optim/lanczos_algorithm_sanity.py  \n",
            "   creating: ./fosi/torch_optim/\n",
            " extracting: ./fosi/torch_optim/__init__.py  \n",
            " extracting: ./fosi/torch_optim/extreme_spectrum_estimation.py  \n",
            " extracting: ./fosi/torch_optim/fosi_optimizer.py  \n",
            " extracting: ./fosi/torch_optim/lanczos_algorithm.py  \n",
            " extracting: ./fosi/torch_optim/lanczos_algorithm_sanity.py  \n",
            " extracting: ./fosi/version.py       \n"
          ]
        }
      ],
      "source": [
        "# ! rm -rf ./fosi/\n",
        "# !mkdir ./fosi/\n",
        "# !unzip fosi.zip -d ./fosi/"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZYKw68tXH0Ue"
      },
      "source": [
        "## Working Example"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZtEnvL7sHwCC",
        "outputId": "27e6e68c-4c93-49e5-e04c-676253a35000"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: torchopt in /usr/local/lib/python3.10/dist-packages (0.7.3)\n",
            "Requirement already satisfied: torch>=1.13 in /usr/local/lib/python3.10/dist-packages (from torchopt) (2.2.1+cu121)\n",
            "Requirement already satisfied: optree>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from torchopt) (0.10.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from torchopt) (1.25.2)\n",
            "Requirement already satisfied: graphviz in /usr/local/lib/python3.10/dist-packages (from torchopt) (0.20.2)\n",
            "Requirement already satisfied: typing-extensions>=4.0.0 in /usr/local/lib/python3.10/dist-packages (from torchopt) (4.10.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch>=1.13->torchopt) (3.13.1)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=1.13->torchopt) (1.12)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.13->torchopt) (3.2.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.13->torchopt) (3.1.3)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch>=1.13->torchopt) (2023.6.0)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch>=1.13->torchopt) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch>=1.13->torchopt) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch>=1.13->torchopt) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==8.9.2.26 in /usr/local/lib/python3.10/dist-packages (from torch>=1.13->torchopt) (8.9.2.26)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in /usr/local/lib/python3.10/dist-packages (from torch>=1.13->torchopt) (12.1.3.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in /usr/local/lib/python3.10/dist-packages (from torch>=1.13->torchopt) (11.0.2.54)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.2.106 in /usr/local/lib/python3.10/dist-packages (from torch>=1.13->torchopt) (10.3.2.106)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in /usr/local/lib/python3.10/dist-packages (from torch>=1.13->torchopt) (11.4.5.107)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in /usr/local/lib/python3.10/dist-packages (from torch>=1.13->torchopt) (12.1.0.106)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.19.3 in /usr/local/lib/python3.10/dist-packages (from torch>=1.13->torchopt) (2.19.3)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch>=1.13->torchopt) (12.1.105)\n",
            "Requirement already satisfied: triton==2.2.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.13->torchopt) (2.2.0)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12 in /usr/local/lib/python3.10/dist-packages (from nvidia-cusolver-cu12==11.4.5.107->torch>=1.13->torchopt) (12.4.99)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.13->torchopt) (2.1.5)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=1.13->torchopt) (1.3.0)\n"
          ]
        }
      ],
      "source": [
        "# !pip install torchopt\n",
        "# !pip install datasets\n",
        "# !pip install evaluate"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "VVIyQm1XFvhQ"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torchvision.transforms as transforms\n",
        "from torch.utils.data.dataloader import default_collate\n",
        "from transformers import AutoModelForSequenceClassification, AutoTokenizer, DataCollatorWithPadding, Trainer, TrainingArguments\n",
        "from torch.optim import Adam\n",
        "import torch.nn as nn\n",
        "import torchopt\n",
        "import functorch\n",
        "import evaluate\n",
        "import torch.nn.functional as F\n",
        "\n",
        "from datasets import load_dataset\n",
        "from fosi import fosi_adam_torch\n",
        "\n",
        "# # Set device\n",
        "# device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
        "\n",
        "# # Load pre-trained DistilBERT model and tokenizer\n",
        "# base_model = AutoModelForSequenceClassification.from_pretrained('bert-base-uncased', num_labels=2)\n",
        "# base_model.to(device)\n",
        "# tokenizer = AutoTokenizer.from_pretrained('bert-base-uncased')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {
        "id": "vB2RtawpGA8R"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "BertClassifier(\n",
              "  (bert): BertForSequenceClassification(\n",
              "    (bert): BertModel(\n",
              "      (embeddings): BertEmbeddings(\n",
              "        (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
              "        (position_embeddings): Embedding(512, 768)\n",
              "        (token_type_embeddings): Embedding(2, 768)\n",
              "        (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "        (dropout): Dropout(p=0.1, inplace=False)\n",
              "      )\n",
              "      (encoder): BertEncoder(\n",
              "        (layer): ModuleList(\n",
              "          (0-11): 12 x BertLayer(\n",
              "            (attention): BertAttention(\n",
              "              (self): BertSelfAttention(\n",
              "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "                (dropout): Dropout(p=0.1, inplace=False)\n",
              "              )\n",
              "              (output): BertSelfOutput(\n",
              "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "                (dropout): Dropout(p=0.1, inplace=False)\n",
              "              )\n",
              "            )\n",
              "            (intermediate): BertIntermediate(\n",
              "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "              (intermediate_act_fn): GELUActivation()\n",
              "            )\n",
              "            (output): BertOutput(\n",
              "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "        )\n",
              "      )\n",
              "      (pooler): BertPooler(\n",
              "        (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "        (activation): Tanh()\n",
              "      )\n",
              "    )\n",
              "    (dropout): Dropout(p=0.1, inplace=False)\n",
              "    (classifier): Linear(in_features=768, out_features=1, bias=True)\n",
              "  )\n",
              "  (sigmoid): Sigmoid()\n",
              ")"
            ]
          },
          "execution_count": 34,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from transformers import AutoModelForSequenceClassification, AutoTokenizer, DataCollatorWithPadding\n",
        "\n",
        "# Load pre-trained BERT model and tokenizer\n",
        "bert_model_name = \"bert-base-uncased\"\n",
        "num_classes = 1\n",
        "bert_model = AutoModelForSequenceClassification.from_pretrained(bert_model_name, num_labels=num_classes)\n",
        "tokenizer = AutoTokenizer.from_pretrained(bert_model_name)\n",
        "\n",
        "# # Freeze all parameters of the BERT model except for the last layer (classifier)\n",
        "# for name, param in bert_model.named_parameters():\n",
        "#     if 'classifier' not in name:  # Exclude parameters of the classifier layer\n",
        "#         param.requires_grad = False\n",
        "\n",
        "# Define a dense layer on top of the BERT model for classification\n",
        "class BertClassifier(nn.Module):\n",
        "    def __init__(self, bert_model, num_classes):\n",
        "        super(BertClassifier, self).__init__()\n",
        "        self.bert = bert_model\n",
        "        self.sigmoid = nn.Sigmoid()\n",
        "\n",
        "    def forward(self, input_ids, attention_mask):\n",
        "        outputs = self.bert(input_ids=input_ids, attention_mask=attention_mask).logits\n",
        "        probability = self.sigmoid(outputs.squeeze())\n",
        "        return probability\n",
        "\n",
        "# Instantiate the classifier\n",
        "classifier = BertClassifier(bert_model, num_classes)\n",
        "\n",
        "# Optionally, move the model to a GPU device if available\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "classifier.to(device)\n",
        "\n",
        "# Now you can train the entire model (BERT + dense layer) on your task-specific data\n",
        "# Make sure to prepare your data (input_ids, attention_mask, labels) using the BERT tokenizer\n",
        "# and use an appropriate loss function and optimizer for your task\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "id": "NEhHBnRiFvhR"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Map: 100%|██████████| 872/872 [00:00<00:00, 8629.73 examples/s]\n"
          ]
        }
      ],
      "source": [
        "\n",
        "# Define a function to preprocess the dataset\n",
        "def prepare_dataset(example):\n",
        "    return tokenizer(example['sentence'], add_special_tokens=True, truncation=True, padding=True, return_tensors='pt')\n",
        "\n",
        "data_collator = DataCollatorWithPadding(tokenizer=tokenizer, padding=True)\n",
        "\n",
        "dataset = load_dataset('glue', 'sst2').map(prepare_dataset, batched=True)\n",
        "metric = evaluate.load(\"glue\", \"sst2\")\n",
        "\n",
        "# Split dataset into train and test sets, we use the train category because the test one has labels -1 only.\n",
        "train_dataset = dataset['train'].select(range(0,500)).remove_columns(['sentence', 'idx']).rename_column('label', 'labels')\n",
        "test_dataset = dataset['train'].select(range(500, 1000)).remove_columns(['sentence', 'idx']).rename_column('label', 'labels')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "7zU_KRNimwR1"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "67349"
            ]
          },
          "execution_count": 7,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "len(dataset['train']['label'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "id": "KaC8HrwiFvhS"
      },
      "outputs": [],
      "source": [
        "# Define data loaders\n",
        "batch_size = 128\n",
        "trainloader = torch.utils.data.DataLoader(train_dataset, batch_size=batch_size, shuffle=True, collate_fn=data_collator)\n",
        "testloader = torch.utils.data.DataLoader(test_dataset, batch_size=batch_size, shuffle=True, collate_fn=data_collator)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "id": "ikDlQ1UDJYGT"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor([[ 101, 2980, 2006,  ...,    0,    0,    0],\n",
            "        [ 101, 2149, 1037,  ...,    0,    0,    0],\n",
            "        [ 101, 1037, 7815,  ...,    0,    0,    0],\n",
            "        ...,\n",
            "        [ 101, 2019, 7736,  ...,    0,    0,    0],\n",
            "        [ 101, 8101, 2319,  ...,    0,    0,    0],\n",
            "        [ 101, 1045, 2079,  ...,    0,    0,    0]])\n"
          ]
        }
      ],
      "source": [
        "for i in (testloader):\n",
        "  print(i['input_ids'])\n",
        "  break"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "id": "TO8H7XfbFvhT"
      },
      "outputs": [],
      "source": [
        "# With buffers\n",
        "\n",
        "def loss_fn(functional_model, params, buffers, input_ids, attention_mask, labels):\n",
        "    preds = functional_model(params, buffers=buffers, input_ids=input_ids, attention_mask=attention_mask)\n",
        "    loss = nn.functional.binary_cross_entropy(preds.squeeze().to(torch.float32), labels.squeeze().to(torch.float32))\n",
        "    return loss\n",
        "\n",
        "# def softmax_output_fn(params, buffers, input_ids, attention_mask, labels):\n",
        "#     logits = model(params, buffers=buffers, input_ids=input_ids, attention_mask=attention_mask, labels=labels).logits\n",
        "#     logits_tensor = torch.tensor(logits)\n",
        "#     softmax_output = F.softmax(logits_tensor, dim=1)\n",
        "#     return softmax_output"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sRrhbO-bIJDN"
      },
      "outputs": [],
      "source": [
        "# Without buffers\n",
        "\n",
        "# def loss_fn(params, input_ids, attention_mask, labels):\n",
        "#     loss = model(params, input_ids=input_ids, attention_mask=attention_mask, labels=labels).loss\n",
        "#     return loss\n",
        "\n",
        "# def softmax_output_fn(params, input_ids, attention_mask, labels):\n",
        "#     logits = model(params, input_ids=input_ids, attention_mask=attention_mask, labels=labels).logits\n",
        "#     logits_tensor = torch.tensor(logits)\n",
        "#     softmax_output = F.softmax(logits_tensor, dim=1)\n",
        "#     return softmax_output"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "metadata": {
        "id": "QU6tGV1toSnb"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "****************************************************************************************************\n",
            "\n",
            "\n",
            "input_ids: tensor([[  101,  1996,  1010,  ...,     0,     0,     0],\n",
            "        [  101,  8579,  4244,  ...,     0,     0,     0],\n",
            "        [  101,  3288, 14095,  ...,     0,     0,     0],\n",
            "        ...,\n",
            "        [  101,  2387,  2129,  ...,     0,     0,     0],\n",
            "        [  101,  2079,  5051,  ...,     0,     0,     0],\n",
            "        [  101,  1996,  2466,  ...,     0,     0,     0]])\n",
            "attention_mask: tensor([[1, 1, 1,  ..., 0, 0, 0],\n",
            "        [1, 1, 1,  ..., 0, 0, 0],\n",
            "        [1, 1, 1,  ..., 0, 0, 0],\n",
            "        ...,\n",
            "        [1, 1, 1,  ..., 0, 0, 0],\n",
            "        [1, 1, 1,  ..., 0, 0, 0],\n",
            "        [1, 1, 1,  ..., 0, 0, 0]])\n",
            "labels: tensor([0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1,\n",
            "        0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1,\n",
            "        1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0,\n",
            "        1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1,\n",
            "        1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0,\n",
            "        0, 0, 1, 0, 1, 0, 1, 0])\n",
            "Returned ESE function. Lanczos order (m) is 20 .\n",
            "Labels: \n",
            "tensor([0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1,\n",
            "        0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1,\n",
            "        1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0,\n",
            "        1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1,\n",
            "        1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0,\n",
            "        0, 0, 1, 0, 1, 0, 1, 0])\n",
            "\n",
            "Step: 0\n",
            "\n",
            "Loss: 0.6983643174171448\n",
            "\n",
            "\n",
            "\n",
            "****************************************************************************************************\n",
            "\n",
            "\n",
            "Labels: \n",
            "tensor([1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1,\n",
            "        1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1,\n",
            "        1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0,\n",
            "        1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1,\n",
            "        0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1,\n",
            "        1, 1, 0, 0, 1, 1, 0, 1])\n",
            "\n",
            "Step: 1\n",
            "\n",
            "Loss: 2.0319175720214844\n",
            "\n",
            "\n",
            "\n",
            "****************************************************************************************************\n",
            "\n",
            "\n",
            "Labels: \n",
            "tensor([0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1,\n",
            "        1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1,\n",
            "        0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0,\n",
            "        0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1,\n",
            "        1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1,\n",
            "        1, 1, 0, 0, 1, 1, 1, 1])\n",
            "\n",
            "Step: 2\n",
            "\n",
            "Loss: 3.689037799835205\n",
            "\n",
            "\n",
            "\n",
            "****************************************************************************************************\n",
            "\n",
            "\n",
            "Labels: \n",
            "tensor([1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0,\n",
            "        0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1,\n",
            "        0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1,\n",
            "        0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0,\n",
            "        1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1])\n",
            "\n",
            "Step: 3\n",
            "\n",
            "Loss: 3.0179452896118164\n",
            "\n",
            "\n",
            "\n",
            "****************************************************************************************************\n",
            "\n",
            "\n",
            "Labels: \n",
            "tensor([1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1,\n",
            "        1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0,\n",
            "        1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1,\n",
            "        1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1,\n",
            "        0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1,\n",
            "        1, 1, 0, 0, 1, 0, 1, 1])\n",
            "\n",
            "Step: 0\n",
            "\n",
            "Loss: 2.271528959274292\n",
            "\n",
            "\n",
            "\n",
            "****************************************************************************************************\n",
            "\n",
            "\n",
            "Labels: \n",
            "tensor([1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1,\n",
            "        0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1,\n",
            "        0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0,\n",
            "        0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0,\n",
            "        1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1,\n",
            "        0, 1, 1, 1, 0, 0, 0, 0])\n",
            "\n",
            "Step: 1\n",
            "\n",
            "Loss: 0.846075177192688\n",
            "\n",
            "\n",
            "\n",
            "****************************************************************************************************\n",
            "\n",
            "\n",
            "Labels: \n",
            "tensor([0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0,\n",
            "        0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1,\n",
            "        0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1,\n",
            "        1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0,\n",
            "        0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1,\n",
            "        0, 1, 0, 1, 0, 1, 1, 0])\n",
            "\n",
            "Step: 2\n",
            "\n",
            "Loss: 1.3782180547714233\n",
            "\n",
            "\n",
            "\n",
            "****************************************************************************************************\n",
            "\n",
            "\n",
            "Labels: \n",
            "tensor([1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0,\n",
            "        0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0,\n",
            "        0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1,\n",
            "        1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0,\n",
            "        1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1])\n",
            "\n",
            "Step: 3\n",
            "\n",
            "Loss: 0.767341136932373\n",
            "\n",
            "Epoch: 1\n",
            "Results: \n",
            "{'accuracy': 0.566}\n",
            "\n",
            "Finished Training\n"
          ]
        }
      ],
      "source": [
        "# Train the model\n",
        "classifier.train()\n",
        "for epoch in range(2):\n",
        "    for i, data in enumerate(trainloader, 0):\n",
        "        print(\"\\n\")\n",
        "        print(\"*\"*100)\n",
        "        print(\"\\n\")\n",
        "\n",
        "        if i == 0 and epoch == 0: # Initialize optimizer and model parameters\n",
        "            print(f\"input_ids: {data['input_ids']}\")\n",
        "            print(f\"attention_mask: {data['attention_mask']}\")\n",
        "            print(f\"labels: {data['labels']}\")\n",
        "            # Define optimizer\n",
        "            base_optimizer = torchopt.adam(lr=0.01)\n",
        "            optimizer = fosi_adam_torch(base_optimizer, loss_fn, data, num_iters_to_approx_eigs=500, alpha=0.01)\n",
        "            # func_optimizer = torchopt.FuncOptimizer(fosi_adam_torch(base_optimizer, loss_fn, data, num_iters_to_approx_eigs=500, alpha=0.01))\n",
        "            model, params, buffers = functorch.make_functional_with_buffers(model=classifier)\n",
        "            opt_state = optimizer.init(params)\n",
        "\n",
        "        input_ids = data['input_ids'].squeeze().to(device)\n",
        "        attention_mask = data['attention_mask'].squeeze().to(device)\n",
        "        labels = data['labels'].squeeze().to(device)\n",
        "        print(f\"Labels: \\n{labels}\\n\")\n",
        "\n",
        "        loss = loss_fn(functional_model=model, \n",
        "                       params=params, buffers=buffers, input_ids=input_ids,attention_mask=attention_mask, labels=labels)\n",
        "        print(f\"Step: {i}\\n\")\n",
        "        print(f\"Loss: {loss}\\n\")\n",
        "\n",
        "        # func_optimizer.step(loss, params, inplace=True)\n",
        "\n",
        "        # Calculate gradients\n",
        "        grads = torch.autograd.grad(loss, params)\n",
        "\n",
        "        # Update model parameters\n",
        "        updates, opt_state = optimizer.update(grads, opt_state, params)\n",
        "        params = torchopt.apply_updates(params, updates, inplace=True)\n",
        "\n",
        "        # Should this method exists? Are buffers neseccary to update?\n",
        "        # Update buffers based on updated parameters\n",
        "        # buffers = torchopt.update_buffers(model, params, buffers)\n",
        "    evaluation_results = []\n",
        "\n",
        "    # Evaluate the model\n",
        "    classifier.eval()\n",
        "    with torch.no_grad():\n",
        "        for i, data in enumerate(testloader):\n",
        "            evaluation_result = {}\n",
        "\n",
        "            input_ids = data['input_ids'].squeeze().to(device)\n",
        "            attention_mask = data['attention_mask'].squeeze().to(device)\n",
        "            labels = data['labels'].squeeze().to(device)\n",
        "\n",
        "            preds = model(params, buffers, input_ids, attention_mask=attention_mask)\n",
        "\n",
        "            predicted_labels = torch.round(preds)\n",
        "\n",
        "            # Save the evaluation results\n",
        "            evaluation_result['input_ids'] = input_ids.cpu().tolist()\n",
        "            evaluation_result['attention_mask'] = attention_mask.cpu().tolist()\n",
        "            evaluation_result['labels'] = labels.cpu().tolist()\n",
        "            evaluation_result['preds'] = preds.cpu().tolist()\n",
        "            evaluation_result['predicted_label'] = predicted_labels.cpu().tolist()\n",
        "\n",
        "            evaluation_results.append(evaluation_result)\n",
        "            metric.add_batch(predictions=predicted_labels, references=labels)\n",
        "\n",
        "print(f'Epoch: {epoch}')\n",
        "results = metric.compute()\n",
        "print(f\"Results: \\n{results}\\n\")\n",
        "\n",
        "print('Finished Training')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0]\n"
          ]
        }
      ],
      "source": [
        "print(evaluation_results[0]['labels'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 42,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "dict_keys(['input_ids', 'attention_mask', 'labels', 'preds', 'predicted_label'])\n"
          ]
        }
      ],
      "source": [
        "print(evaluation_results[0].keys())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 43,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[0.7217391133308411, 0.6180686354637146, 0.6812028884887695, 0.6741484999656677, 0.7031254768371582, 0.7577357888221741, 0.6773779988288879, 0.616105854511261, 0.6453668475151062, 0.6326610445976257, 0.7034841775894165, 0.7196648120880127, 0.6296562552452087, 0.7020332217216492, 0.6612133979797363, 0.5831028819084167, 0.6385987997055054, 0.7122625708580017, 0.6820878386497498, 0.7093048691749573, 0.718841552734375, 0.6197752356529236, 0.6700179576873779, 0.6960123181343079, 0.6805780529975891, 0.6975422501564026, 0.6149519085884094, 0.5524992346763611, 0.7058302760124207, 0.6343899369239807, 0.6984028816223145, 0.6195926666259766, 0.6115147471427917, 0.6497581601142883, 0.5942254662513733, 0.5956348776817322, 0.5560219287872314, 0.58880615234375, 0.7032749056816101, 0.5823532342910767, 0.6333704590797424, 0.6251976490020752, 0.6919683218002319, 0.7145364284515381, 0.7224327921867371, 0.5919395089149475, 0.6705578565597534, 0.7285133004188538, 0.6557971239089966, 0.7236104607582092, 0.6461557745933533, 0.6489565968513489, 0.6546093225479126, 0.7311680316925049, 0.7020087242126465, 0.5944221019744873, 0.707216203212738, 0.6768440008163452, 0.6092169284820557, 0.6915215253829956, 0.6572229266166687, 0.7224683165550232, 0.6149160265922546, 0.6902633309364319, 0.6463732123374939, 0.6731938123703003, 0.6335389614105225, 0.7130426168441772, 0.655922532081604, 0.6977492570877075, 0.6670661568641663, 0.6775104999542236, 0.653333842754364, 0.6931190490722656, 0.5901578664779663, 0.6949876546859741, 0.6178426146507263, 0.7284560203552246, 0.6163551807403564, 0.6790235638618469, 0.626063346862793, 0.6484206318855286, 0.6888652443885803, 0.6840655207633972, 0.6445081233978271, 0.7030940651893616, 0.7207826375961304, 0.6422145962715149, 0.6110559701919556, 0.5463340282440186, 0.6766437292098999, 0.6697548031806946, 0.6386898159980774, 0.6479969620704651, 0.6593778729438782, 0.7167853713035583, 0.7016589641571045, 0.6597579121589661, 0.6718277335166931, 0.6600926518440247, 0.6788516044616699, 0.6991021633148193, 0.6913674473762512, 0.6972963809967041, 0.644589900970459, 0.6817734241485596, 0.6353631615638733, 0.7152928113937378, 0.6097347140312195, 0.6246934533119202, 0.6923813223838806, 0.7339430451393127, 0.6513786315917969, 0.6015394330024719, 0.742753803730011, 0.6433493494987488, 0.5500048398971558, 0.6402838826179504, 0.6815385222434998, 0.6725625991821289, 0.5977705717086792, 0.6100084185600281, 0.6016007661819458, 0.7008025646209717, 0.7393788695335388, 0.6602529287338257, 0.585909903049469, 0.6272488832473755]\n"
          ]
        }
      ],
      "source": [
        "print(evaluation_results[0]['preds'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 44,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0]\n"
          ]
        }
      ],
      "source": [
        "print(evaluation_results[0]['predicted_label'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 45,
      "metadata": {
        "id": "o53j7gDuFdBB"
      },
      "outputs": [],
      "source": [
        "import gc\n",
        "gc.collect() # Python thing\n",
        "torch.cuda.empty_cache() # PyTorch thing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "G0N3gFB0FvhT"
      },
      "outputs": [],
      "source": [
        "# # Train the model\n",
        "# for epoch in range(3):\n",
        "#     for i, data in enumerate(trainloader, 0):\n",
        "#         print(\"\\n\")\n",
        "#         print(\"*\"*100)\n",
        "#         print(\"\\n\")\n",
        "\n",
        "#         if i == 0: # Initialize optimizer and model parameters\n",
        "#             print(f\"input_ids: {data['input_ids']}\")\n",
        "#             print(f\"attention_mask: {data['attention_mask']}\")\n",
        "#             print(f\"labels: {data['labels']}\")\n",
        "#             # Define optimizer\n",
        "#             base_optimizer = torchopt.adam(lr=0.01)\n",
        "#             optimizer = fosi_adam_torch(base_optimizer, loss_fn, data, num_iters_to_approx_eigs=500, alpha=0.01)\n",
        "#             model, params, buffers = functorch.make_functional_with_buffers(model=base_model)\n",
        "#             # model, params = functorch.make_functional(model=base_model)\n",
        "#             opt_state = optimizer.init(params)\n",
        "#             # model.train()\n",
        "\n",
        "\n",
        "#         input_ids = data['input_ids']\n",
        "#         attention_mask = data['attention_mask']\n",
        "#         labels = data['labels']\n",
        "#         print(f\"Labels: \\n{labels}\\n\")\n",
        "\n",
        "#         loss = loss_fn(params, buffers, input_ids, attention_mask, labels)\n",
        "#         # loss = loss_fn(params, input_ids, attention_mask, labels)\n",
        "#         print(f\"Step: {i}\\n\")\n",
        "#         print(f\"Loss: {loss}\\n\")\n",
        "\n",
        "#         print(f\"Calculating Gradients\\n\")\n",
        "#         grads = torch.autograd.grad(loss, params)\n",
        "#         # print(f\"Grads: \\n{grads}\\n\")\n",
        "#         print(f\"Calculating updates in the model...\\n\")\n",
        "#         updates, opt_state = optimizer.update(grads, opt_state, params)\n",
        "#         print(\"Applying updates\\n\")\n",
        "#         params = torchopt.apply_updates(params, updates, inplace=True)\n",
        "\n",
        "#     # Evaluate the model\n",
        "#     # model.eval()\n",
        "#     with torch.no_grad():\n",
        "#         for data in testloader:\n",
        "#             input_ids = data['input_ids'].to(device)\n",
        "#             attention_mask = data['attention_mask'].to(device)\n",
        "#             labels = data['labels'].to(device)\n",
        "\n",
        "#             logits = model(params, buffers, input_ids, attention_mask=attention_mask).logits\n",
        "#             # logits = model(params, input_ids, attention_mask=attention_mask).logits\n",
        "#             print(f\"Logits: {logits}\")\n",
        "#             logits_tensor = torch.tensor(logits)\n",
        "#             softmax_output = F.softmax(logits_tensor, dim=1)\n",
        "\n",
        "#             print(f\"SoftMax of Logits: {softmax_output}\")\n",
        "#             predicted_labels = torch.argmax(softmax_output, dim=1)\n",
        "#             print(f\"Argmax Predictions: {predicted_labels}\")\n",
        "\n",
        "#             metric.add_batch(predictions=predicted_labels, references=labels)\n",
        "\n",
        "\n",
        "#     print(f'Epoch: {epoch}')\n",
        "#     results = metric.compute()\n",
        "#     print(f\"Results: \\n{results}\\n\")\n",
        "#     # print(f'Correct: {correct}\\n')\n",
        "#     # print(f'Total: {total}\\n')\n",
        "#     # print(f'Predicted: {predicted}\\n')\n",
        "#     # print(f'outputs: {outputs}\\n')\n",
        "#     # print(f'labels: {labels}\\n')\n",
        "#     # print(f'Accuracy of the network on the test data: {100 * correct / total}%')\n",
        "#     # model.train() # for the next epoch\n",
        "\n",
        "# print('Finished Training')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NYI0DC4MlAnm"
      },
      "outputs": [],
      "source": [
        "# # Evaluate the model\n",
        "# model.eval()\n",
        "# correct = 0\n",
        "# total = 0\n",
        "# with torch.no_grad():\n",
        "#     for data in testloader:\n",
        "#         input_ids = data['input_ids'].to(device)\n",
        "#         attention_mask = data['attention_mask'].to(device)\n",
        "#         labels = data['labels'].to(device)\n",
        "#         outputs = model(params, buffers, input_ids, attention_mask=attention_mask)\n",
        "#         _, predicted = torch.max(outputs.logits, 1)\n",
        "#         total += labels.size(0)\n",
        "#         correct += (predicted == labels).sum().item()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Itjc8AT9FvhU"
      },
      "outputs": [],
      "source": [
        "# # Define training loop\n",
        "# for epoch in range(3):  # Adjust number of epochs as needed\n",
        "#     model.train()\n",
        "#     for i, batch in enumerate(train_dataset):\n",
        "#         input_ids = batch['input_ids']\n",
        "#         attention_mask = batch['attention_mask']\n",
        "#         labels = batch['label']\n",
        "\n",
        "#         #This is taking care automatically\n",
        "#         # optimizer.zero_grad() This is taking care automatically\n",
        "\n",
        "#         outputs = model(input_ids=input_ids, attention_mask=attention_mask, labels=labels)\n",
        "#         loss = outputs.loss\n",
        "#         loss.backward()\n",
        "#         optimizer.step()\n",
        "\n",
        "#         # if (i + 1) % 100 == 0:\n",
        "#         #     print(f'[{epoch + 1}, {i + 1:5d}] loss: {loss.item():.3f}')\n",
        "\n",
        "#     # Evaluate on test set\n",
        "#     model.eval()\n",
        "#     total = 0\n",
        "#     correct = 0\n",
        "#     with torch.no_grad():\n",
        "#         for batch in test_dataset:\n",
        "#             input_ids = batch['input_ids']\n",
        "#             attention_mask = batch['attention_mask']\n",
        "#             labels = batch['label']\n",
        "#             print(\"Labels: \\n\", labels)\n",
        "\n",
        "#             outputs = model(input_ids, attention_mask=attention_mask, labels=labels)\n",
        "#             logits = outputs.logits\n",
        "#             _, predicted = torch.max(logits, 1)\n",
        "#             total += labels.size(0)\n",
        "#             correct += (predicted == labels).sum().item()\n",
        "\n",
        "#     accuracy = correct / total\n",
        "#     print(f'Test accuracy: {accuracy:.4f}')\n",
        "\n",
        "# print('Finished Training')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VYHLY-DRFvhU"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "include_colab_link": true,
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.2"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
